{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspitballn some ideas here.. basically the concept here is going to be detecting multiple commands within\\na single statement, e.g should detect \"Could you add 12 to the max temperature in London\" as 2 seperate commands of\\n[\"add 12\", \"max temperature in London\"]. \\n\\n# approach 1\\n\\nNN! work -> POS tag -> vec\\n\\ninput -> vecs (dynamic)\\noutput -> pridict val that it is a command\\n\\n# 2\\ninput -> predict of last word, vec of current word\\noutput -> predict of current word\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "spitballn some ideas here.. basically the concept here is going to be detecting multiple commands within\n",
    "a single statement, e.g should detect \"Could you add 12 to the max temperature in London\" as 2 seperate commands of\n",
    "[\"add 12\", \"max temperature in London\"]. \n",
    "\n",
    "# approach 1\n",
    "\n",
    "NN! work -> POS tag -> vec\n",
    "\n",
    "input -> vecs (dynamic)\n",
    "output -> pridict val that it is a command\n",
    "\n",
    "# 2\n",
    "input -> predict of last word, vec of current word\n",
    "output -> predict of current word\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\guyal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\guyal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VB': 3, 'CD': 4, 'NN': 3, 'RP': 1, 'DT': 1, 'NNS': 1, 'RB': 1, 'TO': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import nltk\n",
    "\n",
    "data = pandas.read_json('chain_ds.json')\n",
    "\n",
    "s = {} # freq\n",
    "\n",
    "for f in data['data']:\n",
    "    for r in f['res']:\n",
    "        tok = nltk.word_tokenize(r)\n",
    "        pos = nltk.pos_tag(tok)    \n",
    "\n",
    "        for t in pos:\n",
    "            if t[1] in s:\n",
    "                s[t[1]] += 1\n",
    "            else:\n",
    "                s[t[1]] = 1\n",
    "        \n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MD': 3.923076923076923,\n",
       " 'PRP': 0.9230769230769229,\n",
       " 'VB': 4.076923076923077,\n",
       " 'CD': 4.076923076923077,\n",
       " 'TO': 2.076923076923077,\n",
       " 'DT': 1.923076923076923,\n",
       " 'NN': 0.0769230769230771,\n",
       " 'IN': 0.9230769230769229,\n",
       " 'NNP': 3.923076923076923,\n",
       " 'RP': 1.076923076923077,\n",
       " 'NNS': 1.923076923076923,\n",
       " 'CC': 1.923076923076923,\n",
       " 'RB': 4.076923076923077}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp = {}\n",
    "\n",
    "for f in data['data']:\n",
    "    inp = f['in']\n",
    "    in_tok = nltk.word_tokenize(inp)\n",
    "    in_pos = nltk.pos_tag(in_tok)\n",
    "    \n",
    "    for k in range(len(in_pos)):\n",
    "        c = in_pos[k][1]\n",
    "        prev = \"\" if k == 0 else in_pos[k - 1][1]\n",
    "        nex = \"\" if k == len(in_pos) - 1 else in_pos[k + 1][1]\n",
    "        \n",
    "        c_s = s[c] if c in s else 0\n",
    "        prev_s = s[prev] if prev in s else 0\n",
    "        nex_s = s[nex] if nex in s else 0\n",
    "        \n",
    "        t = c_s + prev_s + nex_s\n",
    "        mp[c] =t\n",
    "        \n",
    "dm = {}\n",
    "\n",
    "mp_v = mp.values() \n",
    "m = sum(mp_v) / len(mp_v)\n",
    "\n",
    "for k in mp:\n",
    "    dm[k] = (abs(mp[k] - m))\n",
    "\n",
    "dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('turn', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('computer', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('my', 'PRP$'),\n",
       " ('toaster', 'NN')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"turn on my computer and my toaster\"\n",
    "\n",
    "t_tok = nltk.word_tokenize(t)\n",
    "t_pos = nltk.pos_tag(t_tok)\n",
    "\n",
    "for w in_tos:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
